{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8899c25e-0321-4f23-8187-e5177aace619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        node_id1   node_id2 label_id1 label_id2   ibd_sum  ibd_n\n",
      "0         node_0     node_5  мордвины  мордвины  29.81720      4\n",
      "1         node_0    node_10  мордвины  мордвины  11.63220      1\n",
      "2         node_0    node_11  мордвины  мордвины  23.90440      2\n",
      "3         node_0    node_18  мордвины  мордвины  11.25290      1\n",
      "4         node_0    node_20  мордвины  мордвины   8.88252      1\n",
      "...          ...        ...       ...       ...       ...    ...\n",
      "67498  node_3741  node_3752  белорусы  белорусы   9.51327      1\n",
      "67499  node_3745  node_3755  белорусы  белорусы   9.23221      1\n",
      "67500  node_3749  node_3764  белорусы  белорусы  10.63310      1\n",
      "67501  node_3754  node_3755  украинцы  белорусы   8.04722      1\n",
      "67502  node_3758  node_3766  белорусы  белорусы   8.77936      1\n",
      "\n",
      "[67503 rows x 6 columns]\n",
      "Unique ids in ibd datafile: 3767\n",
      "OK: Ids are starting from 0\n",
      "OK: Ids are consecutive\n",
      "Label dictionary: {'мордвины': 0, 'белорусы': 1, 'украинцы': 2, 'южные-русские': 3, 'северные-русские': 4}\n",
      "paircount: 67503, unique: 67503\n",
      "OK: all pairs are unique\n"
     ]
    }
   ],
   "source": [
    "#1. prepare dataset for torch batching\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import ibdloader\n",
    "datapath = \"../datasets-genotek/\"\n",
    "\n",
    "dataset1fname = datapath+\"CR_graph_rel.csv\"\n",
    "df = pd.read_csv(dataset1fname)\n",
    "print(df)\n",
    "\n",
    "pairs, weights, labels, labeldict =\\\n",
    "ibdloader.load_pure( dataset1fname )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90ee78f9-4f59-490a-bd90-866896aa2ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test|train selection\n",
    "#10% of every class goes to test\n",
    "#сгруппируем вершины по классам\n",
    "nodeclasses = {}\n",
    "for label in labeldict:\n",
    "    idx = labeldict[label]\n",
    "    nodeclasses[label] = np.argwhere(labels==idx).flatten()\n",
    "   \n",
    "for label in nodeclasses:\n",
    "    print (f\"{label}: {nodeclasses[label].shape[0]}\")\n",
    "\n",
    "\n",
    "nodeclasses_train = {}\n",
    "print (\"train partition\")\n",
    "for label in nodeclasses:\n",
    "    print (f\"{label}: {nodeclasses_train[label].shape[0]}\")\n",
    "nodeclasses_test = {}\n",
    "print (\"test partition\")\n",
    "for label in nodeclasses:\n",
    "    print (f\"{label}: {nodeclasses_test[label].shape[0]}\")\n",
    "indices_train\n",
    "indices_test\n",
    "train_mask = np.array([True]*self.len)\n",
    "test_mask = np.array([True]*self.len)\n",
    "\n",
    "#experiment 1: simple one-hot encoding of class as 5 features\n",
    "def onehot(lbl):\n",
    "    arr = [0,0,0,0,0]\n",
    "    arr[lbl]=1\n",
    "    return arr\n",
    "        \n",
    "features = [ onehot(label) for label in labels]\n",
    "features = np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "90a4cb42-f8fe-4365-b663-2cb2b7e051b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Data(Dataset):\n",
    "    def __init__(self, X, y, train_mask, test_mask):\n",
    "        self.X = torch.from_numpy(X.astype(np.float32))\n",
    "        self.y = torch.from_numpy(y.astype(np.int64))\n",
    "        self.len = self.X.shape[0]\n",
    "        self.num_features = 5\n",
    "        self.num_classes = 5\n",
    "        self.train_mask = train_mask\n",
    "        self.test_mask = test_mask\n",
    "       \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "   \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "   \n",
    "batch_size = 4\n",
    "\n",
    "# Instantiate training and test data\n",
    "train_data = Data(features, labels, train_mask, test_mask)\n",
    "train_dataloader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8860edf6-c678-48d4-8b74-1cb3b3a59b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (lin1): Linear(in_features=5, out_features=16, bias=True)\n",
      "  (lin2): Linear(in_features=16, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.lin1 = Linear(train_data.num_features, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, train_data.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = x.relu()\n",
    "        #x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        return x\n",
    "\n",
    "model = MLP(hidden_channels=16)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2054987a-872b-4116-93a9-398628c644d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 1.5821\n",
      "Epoch: 002, Loss: 1.5477\n",
      "Epoch: 003, Loss: 1.5139\n",
      "Epoch: 004, Loss: 1.4800\n",
      "Epoch: 005, Loss: 1.4466\n",
      "Epoch: 006, Loss: 1.4135\n",
      "Epoch: 007, Loss: 1.3808\n",
      "Epoch: 008, Loss: 1.3480\n",
      "Epoch: 009, Loss: 1.3150\n",
      "Epoch: 010, Loss: 1.2817\n",
      "Epoch: 011, Loss: 1.2479\n",
      "Epoch: 012, Loss: 1.2137\n",
      "Epoch: 013, Loss: 1.1790\n",
      "Epoch: 014, Loss: 1.1439\n",
      "Epoch: 015, Loss: 1.1085\n",
      "Epoch: 016, Loss: 1.0731\n",
      "Epoch: 017, Loss: 1.0374\n",
      "Epoch: 018, Loss: 1.0016\n",
      "Epoch: 019, Loss: 0.9665\n",
      "Epoch: 020, Loss: 0.9314\n",
      "Epoch: 021, Loss: 0.8965\n",
      "Epoch: 022, Loss: 0.8620\n",
      "Epoch: 023, Loss: 0.8281\n",
      "Epoch: 024, Loss: 0.7949\n",
      "Epoch: 025, Loss: 0.7625\n",
      "Epoch: 026, Loss: 0.7307\n",
      "Epoch: 027, Loss: 0.7000\n",
      "Epoch: 028, Loss: 0.6703\n",
      "Epoch: 029, Loss: 0.6414\n",
      "Epoch: 030, Loss: 0.6148\n",
      "Epoch: 031, Loss: 0.5882\n",
      "Epoch: 032, Loss: 0.5623\n",
      "Epoch: 033, Loss: 0.5368\n",
      "Epoch: 034, Loss: 0.5119\n",
      "Epoch: 035, Loss: 0.4878\n",
      "Epoch: 036, Loss: 0.4643\n",
      "Epoch: 037, Loss: 0.4414\n",
      "Epoch: 038, Loss: 0.4194\n",
      "Epoch: 039, Loss: 0.3981\n",
      "Epoch: 040, Loss: 0.3778\n",
      "Epoch: 041, Loss: 0.3582\n",
      "Epoch: 042, Loss: 0.3393\n",
      "Epoch: 043, Loss: 0.3212\n",
      "Epoch: 044, Loss: 0.3040\n",
      "Epoch: 045, Loss: 0.2880\n",
      "Epoch: 046, Loss: 0.2728\n",
      "Epoch: 047, Loss: 0.2586\n",
      "Epoch: 048, Loss: 0.2451\n",
      "Epoch: 049, Loss: 0.2322\n",
      "Epoch: 050, Loss: 0.2199\n",
      "Epoch: 051, Loss: 0.2084\n",
      "Epoch: 052, Loss: 0.1974\n",
      "Epoch: 053, Loss: 0.1870\n",
      "Epoch: 054, Loss: 0.1771\n",
      "Epoch: 055, Loss: 0.1677\n",
      "Epoch: 056, Loss: 0.1588\n",
      "Epoch: 057, Loss: 0.1503\n",
      "Epoch: 058, Loss: 0.1423\n",
      "Epoch: 059, Loss: 0.1346\n",
      "Epoch: 060, Loss: 0.1272\n",
      "Epoch: 061, Loss: 0.1202\n",
      "Epoch: 062, Loss: 0.1135\n",
      "Epoch: 063, Loss: 0.1072\n",
      "Epoch: 064, Loss: 0.1011\n",
      "Epoch: 065, Loss: 0.0953\n",
      "Epoch: 066, Loss: 0.0897\n",
      "Epoch: 067, Loss: 0.0844\n",
      "Epoch: 068, Loss: 0.0793\n",
      "Epoch: 069, Loss: 0.0744\n",
      "Epoch: 070, Loss: 0.0698\n",
      "Epoch: 071, Loss: 0.0653\n",
      "Epoch: 072, Loss: 0.0611\n",
      "Epoch: 073, Loss: 0.0570\n",
      "Epoch: 074, Loss: 0.0532\n",
      "Epoch: 075, Loss: 0.0495\n",
      "Epoch: 076, Loss: 0.0459\n",
      "Epoch: 077, Loss: 0.0426\n",
      "Epoch: 078, Loss: 0.0394\n",
      "Epoch: 079, Loss: 0.0364\n",
      "Epoch: 080, Loss: 0.0335\n",
      "Epoch: 081, Loss: 0.0309\n",
      "Epoch: 082, Loss: 0.0284\n",
      "Epoch: 083, Loss: 0.0261\n",
      "Epoch: 084, Loss: 0.0241\n",
      "Epoch: 085, Loss: 0.0222\n",
      "Epoch: 086, Loss: 0.0204\n",
      "Epoch: 087, Loss: 0.0189\n",
      "Epoch: 088, Loss: 0.0175\n",
      "Epoch: 089, Loss: 0.0163\n",
      "Epoch: 090, Loss: 0.0152\n",
      "Epoch: 091, Loss: 0.0142\n",
      "Epoch: 092, Loss: 0.0134\n",
      "Epoch: 093, Loss: 0.0126\n",
      "Epoch: 094, Loss: 0.0119\n",
      "Epoch: 095, Loss: 0.0113\n",
      "Epoch: 096, Loss: 0.0108\n",
      "Epoch: 097, Loss: 0.0103\n",
      "Epoch: 098, Loss: 0.0099\n",
      "Epoch: 099, Loss: 0.0094\n",
      "Epoch: 100, Loss: 0.0091\n",
      "Epoch: 101, Loss: 0.0088\n",
      "Epoch: 102, Loss: 0.0085\n",
      "Epoch: 103, Loss: 0.0082\n",
      "Epoch: 104, Loss: 0.0079\n",
      "Epoch: 105, Loss: 0.0077\n",
      "Epoch: 106, Loss: 0.0075\n",
      "Epoch: 107, Loss: 0.0072\n",
      "Epoch: 108, Loss: 0.0071\n",
      "Epoch: 109, Loss: 0.0069\n",
      "Epoch: 110, Loss: 0.0067\n",
      "Epoch: 111, Loss: 0.0065\n",
      "Epoch: 112, Loss: 0.0064\n",
      "Epoch: 113, Loss: 0.0063\n",
      "Epoch: 114, Loss: 0.0061\n",
      "Epoch: 115, Loss: 0.0060\n",
      "Epoch: 116, Loss: 0.0059\n",
      "Epoch: 117, Loss: 0.0058\n",
      "Epoch: 118, Loss: 0.0057\n",
      "Epoch: 119, Loss: 0.0056\n",
      "Epoch: 120, Loss: 0.0055\n",
      "Epoch: 121, Loss: 0.0054\n",
      "Epoch: 122, Loss: 0.0053\n",
      "Epoch: 123, Loss: 0.0052\n",
      "Epoch: 124, Loss: 0.0052\n",
      "Epoch: 125, Loss: 0.0051\n",
      "Epoch: 126, Loss: 0.0050\n",
      "Epoch: 127, Loss: 0.0050\n",
      "Epoch: 128, Loss: 0.0049\n",
      "Epoch: 129, Loss: 0.0049\n",
      "Epoch: 130, Loss: 0.0048\n",
      "Epoch: 131, Loss: 0.0048\n",
      "Epoch: 132, Loss: 0.0047\n",
      "Epoch: 133, Loss: 0.0047\n",
      "Epoch: 134, Loss: 0.0046\n",
      "Epoch: 135, Loss: 0.0046\n",
      "Epoch: 136, Loss: 0.0045\n",
      "Epoch: 137, Loss: 0.0045\n",
      "Epoch: 138, Loss: 0.0045\n",
      "Epoch: 139, Loss: 0.0044\n",
      "Epoch: 140, Loss: 0.0044\n",
      "Epoch: 141, Loss: 0.0043\n",
      "Epoch: 142, Loss: 0.0043\n",
      "Epoch: 143, Loss: 0.0043\n",
      "Epoch: 144, Loss: 0.0042\n",
      "Epoch: 145, Loss: 0.0042\n",
      "Epoch: 146, Loss: 0.0041\n",
      "Epoch: 147, Loss: 0.0041\n",
      "Epoch: 148, Loss: 0.0041\n",
      "Epoch: 149, Loss: 0.0040\n",
      "Epoch: 150, Loss: 0.0040\n",
      "Epoch: 151, Loss: 0.0039\n",
      "Epoch: 152, Loss: 0.0039\n",
      "Epoch: 153, Loss: 0.0038\n",
      "Epoch: 154, Loss: 0.0038\n",
      "Epoch: 155, Loss: 0.0038\n",
      "Epoch: 156, Loss: 0.0037\n",
      "Epoch: 157, Loss: 0.0037\n",
      "Epoch: 158, Loss: 0.0037\n",
      "Epoch: 159, Loss: 0.0036\n",
      "Epoch: 160, Loss: 0.0036\n",
      "Epoch: 161, Loss: 0.0036\n",
      "Epoch: 162, Loss: 0.0035\n",
      "Epoch: 163, Loss: 0.0035\n",
      "Epoch: 164, Loss: 0.0035\n",
      "Epoch: 165, Loss: 0.0034\n",
      "Epoch: 166, Loss: 0.0034\n",
      "Epoch: 167, Loss: 0.0034\n",
      "Epoch: 168, Loss: 0.0034\n",
      "Epoch: 169, Loss: 0.0033\n",
      "Epoch: 170, Loss: 0.0033\n",
      "Epoch: 171, Loss: 0.0033\n",
      "Epoch: 172, Loss: 0.0033\n",
      "Epoch: 173, Loss: 0.0033\n",
      "Epoch: 174, Loss: 0.0032\n",
      "Epoch: 175, Loss: 0.0032\n",
      "Epoch: 176, Loss: 0.0032\n",
      "Epoch: 177, Loss: 0.0032\n",
      "Epoch: 178, Loss: 0.0032\n",
      "Epoch: 179, Loss: 0.0032\n",
      "Epoch: 180, Loss: 0.0031\n",
      "Epoch: 181, Loss: 0.0031\n",
      "Epoch: 182, Loss: 0.0031\n",
      "Epoch: 183, Loss: 0.0031\n",
      "Epoch: 184, Loss: 0.0031\n",
      "Epoch: 185, Loss: 0.0031\n",
      "Epoch: 186, Loss: 0.0031\n",
      "Epoch: 187, Loss: 0.0030\n",
      "Epoch: 188, Loss: 0.0030\n",
      "Epoch: 189, Loss: 0.0030\n",
      "Epoch: 190, Loss: 0.0030\n",
      "Epoch: 191, Loss: 0.0030\n",
      "Epoch: 192, Loss: 0.0030\n",
      "Epoch: 193, Loss: 0.0030\n",
      "Epoch: 194, Loss: 0.0030\n",
      "Epoch: 195, Loss: 0.0030\n",
      "Epoch: 196, Loss: 0.0029\n",
      "Epoch: 197, Loss: 0.0029\n",
      "Epoch: 198, Loss: 0.0029\n",
      "Epoch: 199, Loss: 0.0029\n",
      "Epoch: 200, Loss: 0.0029\n"
     ]
    }
   ],
   "source": [
    "model = MLP(hidden_channels=16)\n",
    "criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)  # Define optimizer.\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()  # Clear gradients.\n",
    "    out = model(train_data.X)  # Perform a single forward pass.\n",
    "    loss = criterion(out[train_data.train_mask], train_data.y[train_data.train_mask])  # Compute the loss solely based on the training nodes.\n",
    "    #loss = criterion(out, train_data.y)  # Compute the loss solely based on the training nodes.\n",
    "    \n",
    "    loss.backward()  # Derive gradients.\n",
    "    optimizer.step()  # Update parameters based on gradients.\n",
    "    return loss\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    out = model(train_data.X)\n",
    "    pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "    test_correct = pred[train_data.test_mask] == train_data.y[train_data.test_mask]  # Check against ground-truth labels.\n",
    "    \n",
    "    test_acc = int(test_correct.sum()) / int(train_data.test_mask.sum())  # Derive ratio of correct predictions.\n",
    "    print(f\"Test accuracy: {test_acc}, correct {int(test_correct.sum())} out of {int(train_data.test_mask.sum())}\")\n",
    "    \n",
    "    return test_acc\n",
    "\n",
    "for epoch in range(1, 201):\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "61e062dd-38f1-4ecc-ad8e-d0386a530370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 1.0, correct 3767 out of 3767\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "acc = test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9b090313-0046-4131-bb15-0f1a5afb0d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.X[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87059847-8f72-4a3e-9aa5-0bab9a98363c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
