{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec51752d-a881-452e-856c-dbba714f480e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import ibdloader\n",
    "import baseheuristic as bh\n",
    "\n",
    "rng = np.random.default_rng(2023)\n",
    "\n",
    "datapath = \"../datasets-genotek/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a10f289-95fb-4dd9-8efa-6e0d89d13ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset 1: \n",
    "#only pure population samples and total ibd length and count between them\n",
    "\n",
    "#CR_graph_rel.csv\n",
    "#node_id1,node_id2,label_id1,label_id2,ibd_sum,ibd_n\n",
    "#node_0,node_5,мордвины,мордвины,29.8172,4\n",
    "\n",
    "#NC_graph_rel.csv \n",
    "#node_id1,node_id2,label_id1,label_id2,ibd_sum,ibd_n\n",
    "#node_0,node_1,чеченцы,чеченцы,9.76537,1\n",
    "#node_1,node_138,чеченцы,\"кабардинцы,черкесы,адыгейцы\",8.01344,1\n",
    "\n",
    "dataset1fname = datapath+\"CR_graph_rel.csv\"\n",
    "df = pd.read_csv(dataset1fname)\n",
    "print(df)\n",
    "\n",
    "pairs, weights, labels, labeldict, idxtranslator =\\\n",
    "ibdloader.load_pure( dataset1fname )\n",
    "\n",
    "\n",
    "\n",
    "#data struсture conventions:\n",
    "#node indices in datasets are not necessarily starting from 0 and consecutive \n",
    "#so they are enumerated in idxtranslator.\n",
    "#for every idx the label of node with index idxtranslator[idx] is stored in labels[idx]\n",
    "#length of both arrays labels and idxtranslator coincides with count of available nodes in the dataset  \n",
    "\n",
    "#in graphs we store consecutive labels starting from 0\n",
    "#it can be even subset of idxtranslator, so every graph must have its own gr_idxtranslator\n",
    "#and possibly gr_labels\n",
    "\n",
    "#example\n",
    "#idxtranslator = [0 1 3 5 10]\n",
    "#       labels = [2 0 2 0 2]\n",
    "# pairs = [0 10 100]\n",
    "#         [1 5 100]\n",
    "#         [3 10 100]\n",
    "#subset after removing node_3\n",
    "#idxtranslator = [0 1 5 10]\n",
    "#       labels = [2 0 0 2]\n",
    "\n",
    "#lets not produce translated pairs\n",
    "#just graphs with corresponding edges and nodes from 0 and consecutive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2d2889-0dbe-4b76-a711-afb43bdbbce1",
   "metadata": {},
   "source": [
    "# 1. построим распределение суммарных весов и количества ibd-сегментов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d06a26-c22f-4127-9f2a-34db41b95884",
   "metadata": {},
   "outputs": [],
   "source": [
    "bh.plot_distribution(weights, 50, \"Distribution of ibd sum\")\n",
    "bh.plot_distribution(pairs[:,2], 10, \"Distribution of ibd count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87f6b72-af99-4480-8354-c683e5e95f3b",
   "metadata": {},
   "source": [
    "# 2. Распределение весов внутри и между классами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3254de40-5a01-4730-bc75-d3ef225a3889",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#change plot_distr to true to plot distributions\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "graphdata = bh.composegraphs(pairs, weights, labels, labeldict, idxtranslator)\n",
    "ncls = graphdata[0]['nodeclasses']\n",
    "grph = graphdata[0]['graph']\n",
    "trns = graphdata[0]['translation']\n",
    "weimatrix, countmatrix, distrs = bh.getweightandcountmatrices(grph, ncls, labeldict, plot_distr=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d283efa0-bbb3-43a4-a919-e1f63d0520d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dst in distrs:\n",
    "    print(dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9945258d-cb25-440b-81bf-e5ea984e47cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optional: fit distributions to popular pdf curves\n",
    "#distributions here are of ibd sum from individual of one class to the whole another class, so they are\n",
    "#not symmetric, e.g. distribution from mordvins to belarussians is not the same as distribution from belarussians to mordvins\n",
    "\n",
    "#it is isteresting to see symmetric distribution as well (distribution on ibd-sum of one link between two classes)\n",
    "#or distribution of link count from individual of one population to the whole another population (also non-symmetrical)\n",
    "\n",
    "#total distribution of link count between two individuals is not informative (almost constant 1)\n",
    "\n",
    "\n",
    "#dist_names = ['alpha', 'gamma', 'norm']\n",
    "dist_names = ['gamma', 'norm']\n",
    "for label in distrs:\n",
    "    #label = \"южные-русские\"\n",
    "    data = distrs[label][\"data\"]\n",
    "    threshold = distrs[label][\"threshold\"]\n",
    "    if threshold is None:\n",
    "        threshold = 1000\n",
    "    bins = distrs[label][\"bins\"]\n",
    "    bh.plot_and_fit_distribution(data, threshold, bins, f\"Distribution of ibd sum for {label}\", dist_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87beb807-d28a-4542-9ec0-f9b9874f31af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22797073-a5c2-4238-95ce-60435f4359b6",
   "metadata": {},
   "source": [
    "# 3. Матрица сумм весов внутри и между классами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09290c02-41a3-40b7-bef1-2c269f942c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check total ibd sum between classes\n",
    "for label in ncls:\n",
    "    print (f\"{label}: {ncls[label].shape[0]}\")\n",
    "    \n",
    "# в тысячах\n",
    "cm_display = ConfusionMatrixDisplay(weimatrix/1000, display_labels=labeldict.keys()).plot()\n",
    "cm_display.ax_.set_title(\"Веса/1000\")\n",
    "plt.show()\n",
    "cm_display = ConfusionMatrixDisplay(countmatrix, display_labels=labeldict.keys()).plot()\n",
    "cm_display.ax_.set_title(\"Количество ребер\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1865679-8bce-4492-bcbd-5b52dbeb7d40",
   "metadata": {},
   "source": [
    "# Классификация\n",
    "\n",
    "Построим матрицы ошибок по:\n",
    "\n",
    "1. количеству ребер до известных классов\n",
    "2. числу ребер на количество членов класса\n",
    "3. количеству ibd-сегментов до известных классов\n",
    "4. самому длинный сегмент\n",
    "5. весу на ребро\n",
    "6. суммарному весу ребер до известных классов\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efba4bf-5d01-4481-b7ee-5102728313c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "featuredict = bh.getfeatures(grph, grph.nodes, ncls, labeldict, pairs, trns )\n",
    "print(f\"features collected in {time.time()-start} seconds\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2179ce62-a549-49c3-86df-0ebf55306f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "simplepredictions = bh.getsimplepred(featuredict)\n",
    "for feature in featuredict:\n",
    "    prediction = simplepredictions[feature]\n",
    "    title = featuredict[feature][\"comment\"]\n",
    "    bh.show_prediction(labels, prediction, labeldict.keys(), title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfe1219-6d42-4f29-8a6f-4f05427a6412",
   "metadata": {},
   "source": [
    "# Наиболее уверенный классификатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73acb718-eb5b-46dd-a152-d55f022ce772",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#наиболее уверенный классификатор среди нескольких с весами\n",
    "#nodecount = labels.shape[0]\n",
    "#labelcount = len(labeldict)\n",
    "\n",
    "\n",
    "featureweights = {\n",
    "    \"SegmentCount\": 0,\n",
    "    \"SegmentCountPerClassize\": 1.38,\n",
    "    \"SegmentCountWMult\": 0,\n",
    "    \"LongestIbd\": 1,\n",
    "    \"IbdSum\": 0.41,\n",
    "    \"IbdSumPerEdge\": 0\n",
    "}\n",
    "\n",
    "#this is useful for the case where no second-best class is available, so several classifiers have infinite confidence\n",
    "featurepriority = [\n",
    "    \"LongestIbd\",\n",
    "    \"SegmentCountPerClassize\",\n",
    "    \"IbdSum\",\n",
    "    \"SegmentCountWMult\",\n",
    "    \"SegmentCount\",    \n",
    "    \"IbdSumPerEdge\"]\n",
    "\n",
    "mostconfident = bh.get_most_confident_prediction(featuredict, featureweights, featurepriority)\n",
    "bh.show_prediction(labels, mostconfident, labeldict.keys(), \"Классификация наиболее уверенным классификатором.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a4b1a8-3aac-481f-b3af-0698c73789c6",
   "metadata": {},
   "source": [
    "# Простой перебор для поиска наилучшей комбинации весов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f935421c-310c-4466-94eb-dee23d4e2fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#search for the best parameter combination\n",
    "#may take hours\n",
    "featureweightrange = {\n",
    "    \"SegmentCount\": [0],\n",
    "    \"SegmentCountPerClassize\": np.arange(0.5,1.5,0.1),\n",
    "    \"SegmentCountWMult\": [0],    \n",
    "    \"LongestIbd\": [1],\n",
    "    \"IbdSum\": np.arange(0.1,1.5,0.1),    \n",
    "    \"IbdSumPerEdge\": [0]\n",
    "}\n",
    "\n",
    "bestweights, bestaccuracy = bh.search_best_weights(featuredict, featureweightrange, featurepriority, labels, trainnodes = None, show_intermediate = False)\n",
    "print(bestweights, bestaccuracy)\n",
    "mostconfident = bh.get_most_confident_prediction(featuredict, bestweights, featurepriority)\n",
    "bh.show_prediction(labels, mostconfident, labeldict.keys(), \"Классификация наиболее уверенным классификатором.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb400bc-7e96-4f45-ac27-7d0db5f43f9a",
   "metadata": {},
   "source": [
    "# Разбиение на \"тренировочную\", \"валидационную\" и \"тестовую\" части"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113fce08-a7e5-46ea-93bf-23285f64f0e7",
   "metadata": {},
   "source": [
    "Разделим каждый класс train:val:test = 60:20:20, веса выберем по \"тренировочной\"+\"валидационной\" выборке, метрики посчитаем по \"тестовой\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdda974c-92a0-4b51-93fc-7d7ab91d0cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "valshare = 0.2\n",
    "testshare = 0.2\n",
    "#print(ncls)\n",
    "#remember that ncls are starting from 0 and consecutive\n",
    "#also somewhere we should check connectivity, e.g. if test node have connections with train tree at all\n",
    "\n",
    "permt = bh.getrandompermutation(ncls, rng)\n",
    "#print(permt)\n",
    "trainnodeclasses, valnodeclasses, testnodeclasses = bh.dividetrainvaltest(ncls, valshare, testshare, permt)\n",
    "#here subdivision does not change translation array as we still have the same graph,\n",
    "#but will compute features only for test nodes and based on links to trainnodeclasses only\n",
    "\n",
    "\n",
    "print(\"train|val|test distribution\")\n",
    "for c in trainnodeclasses:\n",
    "    print(c, \":\", trainnodeclasses[c].shape[0], \":\", valnodeclasses[c].shape[0], \":\", testnodeclasses[c].shape[0])\n",
    "\n",
    "trainnodes, valnodes, testnodes = bh.gettrainvaltestnodes(trainnodeclasses, valnodeclasses, testnodeclasses)\n",
    "print(\"train nodes:\", trainnodes.shape[0])\n",
    "print(\"val nodes:\", valnodes.shape[0])\n",
    "print(\"test nodes:\", testnodes.shape[0])\n",
    "\n",
    "testlabels = labels[testnodes]\n",
    "\n",
    "start = time.time()\n",
    "featuredict = bh.getfeatures(grph, testnodes, trainnodeclasses, labeldict, pairs, trns )\n",
    "print(f\"features collected in {time.time()-start} seconds\\n\")\n",
    "\n",
    "simplepredictions = bh.getsimplepred(featuredict)\n",
    "for feature in featuredict:\n",
    "    prediction = simplepredictions[feature]\n",
    "    #predictedlabels = np.array([ prediction[node] for node in testnodes])\n",
    "    predictedlabels = prediction[testnodes]\n",
    "    title = featuredict[feature][\"comment\"]\n",
    "    bh.show_prediction(testlabels, predictedlabels, labeldict.keys(), title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c35c88-81b9-4f8c-927d-7501c8a507d9",
   "metadata": {},
   "source": [
    "# Наиболее уверенный классификатор для 30%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fa8337-bab5-42bf-82f2-7a407e3f6754",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "featureweights = {\n",
    "    'IbdSumPerEdge': 0, \n",
    "    'IbdSum': 0.5, \n",
    "    'LongestIbd': 1, \n",
    "    'SegmentCountWMult': 0, \n",
    "    'SegmentCountPerClassize': 1.3, \n",
    "    'SegmentCount': 0}\n",
    "\n",
    "#this is useful for the case where no second-best class is available, so several classifiers have infinite confidence\n",
    "featurepriority = [\n",
    "    \"LongestIbd\",\n",
    "    \"SegmentCountPerClassize\",\n",
    "    \"IbdSum\",\n",
    "    \"SegmentCountWMult\",\n",
    "    \"SegmentCount\",    \n",
    "    \"IbdSumPerEdge\"]\n",
    "\n",
    "mostconfident = bh.get_most_confident_prediction(featuredict, featureweights, featurepriority)\n",
    "predictedlabels = mostconfident[testnodes]\n",
    "bh.show_prediction(testlabels, predictedlabels, labeldict.keys(), \"Классификация наиболее уверенным классификатором.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10f64a9-a81a-442c-bf96-d2aa3b987f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#search for the best parameter combination\n",
    "#may take hours\n",
    "featureweightrange = {\n",
    "    \"SegmentCount\": [0],\n",
    "    \"SegmentCountPerClassize\": np.arange(0.5,1.5,0.1),\n",
    "    \"SegmentCountWMult\": [0],    \n",
    "    \"LongestIbd\": [1],\n",
    "    \"IbdSum\": np.arange(0.1,1.5,0.1),    \n",
    "    \"IbdSumPerEdge\": [0]\n",
    "}\n",
    "\n",
    "#featureweightrange = {\n",
    "#    \"SegmentCount\": [0],\n",
    "#    \"SegmentCountPerClassize\": [1.38],\n",
    "#    \"SegmentCountWMult\": [0],\n",
    "#    \"LongestIbd\": [1],\n",
    "#    \"IbdSum\": [0.41],\n",
    "#    \"IbdSumPerEdge\": [0]\n",
    "#}\n",
    "\n",
    "\n",
    "#we will search for best weights on train nodes and then apply most confident classifier with these weights to test nodes\n",
    "trainlabels = labels[trainnodes] \n",
    "\n",
    "bestweights, bestaccuracy = bh.search_best_weights(featuredict, featureweightrange, featurepriority, trainlabels, trainnodes, False)\n",
    "print(bestweights, bestaccuracy)\n",
    "\n",
    "mostconfident = bh.get_most_confident_prediction(featuredict, bestweights, featurepriority)\n",
    "bh.show_prediction(testlabels, mostconfident[testnodes], labeldict.keys(), \"Классификация наиболее уверенным классификатором.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52250ff0-da33-4376-9286-da85d5760e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#repeat for new subdivision\n",
    "itercount = 50\n",
    "totalaccr = 0\n",
    "for itr in range(itercount):\n",
    "    permt = bh.getrandompermutation(ncls, rng)\n",
    "    #print(permt)\n",
    "    trainnodeclasses, testnodeclasses = bh.dividetraintest(ncls, testshare, permt)\n",
    "    #here subdivision does not change translation array as we still have the same graph,\n",
    "    #but will compute features only for test nodes and based on links to trainnodeclasses only\n",
    "\n",
    "    #when it will be neccessary to \n",
    "    #print(\"train|test distribution\")\n",
    "    #for c in trainnodeclasses:\n",
    "    #    print(c, \":\", trainnodeclasses[c].shape[0], \":\", testnodeclasses[c].shape[0])\n",
    "\n",
    "    trainnodes, testnodes = bh.gettraintestnodes(trainnodeclasses, testnodeclasses)\n",
    "    #print(\"train nodes:\", trainnodes.shape[0])\n",
    "    #print(\"test nodes:\", testnodes.shape[0])\n",
    "\n",
    "\n",
    "    testlabels = labels[testnodes]\n",
    "    trainlabels = labels[trainnodes] \n",
    "\n",
    "    start = time.time()\n",
    "    featuredict = bh.getfeatures(grph, testnodes, trainnodeclasses, labeldict, pairs, trns )\n",
    "    #print(f\"features collected in {time.time()-start} seconds\\n\")\n",
    "\n",
    "    bestweights, bestaccuracy = bh.search_best_weights(featuredict, featureweightrange, featurepriority, trainlabels, trainnodes, False)\n",
    "    print(\"iter\", itr)\n",
    "    print(bestweights, bestaccuracy)\n",
    "    mostconfident = bh.get_most_confident_prediction(featuredict, bestweights, featurepriority)\n",
    "    accr = np.sum(testlabels == mostconfident[testnodes])/testnodes.shape[0]\n",
    "    totalaccr += accr\n",
    "    print(f\" Accuracy: {accr:.4f}, correct: {np.sum(labels == prediction)}, total: {labels.shape[0]}\")\n",
    "    \n",
    "    #bh.show_prediction(testlabels, mostconfident[testnodes], labeldict.keys(), \"Классификация наиболее уверенным классификатором.\")\n",
    "\n",
    "print(\"average accuracy:\", totalaccr/itercount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407c6841-3ae4-4931-a2a8-09cd82125e15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
